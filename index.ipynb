{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>...</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000ff2bfdfe9</td>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>...</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007255e47698</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>013f2bd269f5</td>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>043ac50845d5</td>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>...</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>044fb8a146ec</td>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>...</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id        AB          AF          AH         AM        AR  \\\n",
       "0  000ff2bfdfe9  0.209377  3109.03329   85.200147  22.394407  8.138688   \n",
       "1  007255e47698  0.145282   978.76416   85.200147  36.968889  8.138688   \n",
       "2  013f2bd269f5  0.470030  2635.10654   85.200147  32.360553  8.138688   \n",
       "3  043ac50845d5  0.252107  3819.65177  120.201618  77.112203  8.138688   \n",
       "4  044fb8a146ec  0.380297  3733.04844   85.200147  14.103738  8.138688   \n",
       "\n",
       "         AX        AY         AZ          BC  ...        FL        FR  \\\n",
       "0  0.699861  0.025578   9.812214    5.555634  ...  7.298162   1.73855   \n",
       "1  3.632190  0.025578  13.517790    1.229900  ...  0.173229   0.49706   \n",
       "2  6.732840  0.025578  12.824570    1.229900  ...  7.709560   0.97556   \n",
       "3  3.685344  0.025578  11.053708    1.229900  ...  6.122162   0.49706   \n",
       "4  3.942255  0.054810   3.396778  102.151980  ...  8.153058  48.50134   \n",
       "\n",
       "         FS         GB          GE            GF         GH         GI  \\\n",
       "0  0.094822  11.339138   72.611063   2003.810319  22.136229  69.834944   \n",
       "1  0.568932   9.292698   72.611063  27981.562750  29.135430  32.131996   \n",
       "2  1.198821  37.077772   88.609437  13676.957810  28.022851  35.192676   \n",
       "3  0.284466  18.529584   82.416803   2094.262452  39.948656  90.493248   \n",
       "4  0.121914  16.408728  146.109943   8524.370502  45.381316  36.262628   \n",
       "\n",
       "          GL  Class  \n",
       "0   0.120343      1  \n",
       "1  21.978000      0  \n",
       "2   0.196941      0  \n",
       "3   0.155829      0  \n",
       "4   0.096614      1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    509\n",
       "1    108\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_df = df.Class.value_counts()\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 1 - Has one or many of three conditions (108)\n",
    "Class 0 - None of the three medical conditions(509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>...</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>...</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>...</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>...</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>...</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AB          AF          AH         AM        AR        AX        AY  \\\n",
       "0  0.209377  3109.03329   85.200147  22.394407  8.138688  0.699861  0.025578   \n",
       "1  0.145282   978.76416   85.200147  36.968889  8.138688  3.632190  0.025578   \n",
       "2  0.470030  2635.10654   85.200147  32.360553  8.138688  6.732840  0.025578   \n",
       "3  0.252107  3819.65177  120.201618  77.112203  8.138688  3.685344  0.025578   \n",
       "4  0.380297  3733.04844   85.200147  14.103738  8.138688  3.942255  0.054810   \n",
       "\n",
       "          AZ          BC         BD   ...         FI        FL        FR  \\\n",
       "0   9.812214    5.555634  4126.58731  ...   3.583450  7.298162   1.73855   \n",
       "1  13.517790    1.229900  5496.92824  ...  10.358927  0.173229   0.49706   \n",
       "2  12.824570    1.229900  5135.78024  ...  11.626917  7.709560   0.97556   \n",
       "3  11.053708    1.229900  4169.67738  ...  14.852022  6.122162   0.49706   \n",
       "4   3.396778  102.151980  5728.73412  ...  13.666727  8.153058  48.50134   \n",
       "\n",
       "         FS         GB          GE            GF         GH         GI  \\\n",
       "0  0.094822  11.339138   72.611063   2003.810319  22.136229  69.834944   \n",
       "1  0.568932   9.292698   72.611063  27981.562750  29.135430  32.131996   \n",
       "2  1.198821  37.077772   88.609437  13676.957810  28.022851  35.192676   \n",
       "3  0.284466  18.529584   82.416803   2094.262452  39.948656  90.493248   \n",
       "4  0.121914  16.408728  146.109943   8524.370502  45.381316  36.262628   \n",
       "\n",
       "          GL  \n",
       "0   0.120343  \n",
       "1  21.978000  \n",
       "2   0.196941  \n",
       "3   0.155829  \n",
       "4   0.096614  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping ID column as it will not be used \n",
    "#also drop the predictor \n",
    "\n",
    "train_data=df.drop(columns=['Id','Class'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FI</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>...</td>\n",
       "      <td>3.583450</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>...</td>\n",
       "      <td>10.358927</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>...</td>\n",
       "      <td>11.626917</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>...</td>\n",
       "      <td>14.852022</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>...</td>\n",
       "      <td>13.666727</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.149555</td>\n",
       "      <td>3130.05946</td>\n",
       "      <td>123.763599</td>\n",
       "      <td>9.513984</td>\n",
       "      <td>13.020852</td>\n",
       "      <td>3.499305</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>8.545512</td>\n",
       "      <td>2.804172</td>\n",
       "      <td>4157.68439</td>\n",
       "      <td>...</td>\n",
       "      <td>9.879296</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.26092</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>8.967128</td>\n",
       "      <td>217.148554</td>\n",
       "      <td>8095.932828</td>\n",
       "      <td>24.640462</td>\n",
       "      <td>69.191944</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.435846</td>\n",
       "      <td>5462.03438</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>46.551007</td>\n",
       "      <td>15.973224</td>\n",
       "      <td>5.979825</td>\n",
       "      <td>0.025882</td>\n",
       "      <td>12.622906</td>\n",
       "      <td>3.777550</td>\n",
       "      <td>5654.07556</td>\n",
       "      <td>...</td>\n",
       "      <td>10.910227</td>\n",
       "      <td>10.223150</td>\n",
       "      <td>1.24236</td>\n",
       "      <td>0.426699</td>\n",
       "      <td>35.896418</td>\n",
       "      <td>496.994214</td>\n",
       "      <td>3085.308063</td>\n",
       "      <td>29.648928</td>\n",
       "      <td>124.808872</td>\n",
       "      <td>0.145340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>0.427300</td>\n",
       "      <td>2459.10720</td>\n",
       "      <td>130.138587</td>\n",
       "      <td>55.355778</td>\n",
       "      <td>10.005552</td>\n",
       "      <td>8.070549</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>15.408390</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5888.87769</td>\n",
       "      <td>...</td>\n",
       "      <td>12.029366</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.067730</td>\n",
       "      <td>19.962092</td>\n",
       "      <td>128.896894</td>\n",
       "      <td>6474.652866</td>\n",
       "      <td>26.166072</td>\n",
       "      <td>119.559420</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>0.363205</td>\n",
       "      <td>1263.53524</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>23.685856</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>7.981959</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>7.524588</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4517.86560</td>\n",
       "      <td>...</td>\n",
       "      <td>8.026928</td>\n",
       "      <td>9.256996</td>\n",
       "      <td>0.78764</td>\n",
       "      <td>0.670527</td>\n",
       "      <td>24.594488</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>1965.343176</td>\n",
       "      <td>25.116750</td>\n",
       "      <td>37.155112</td>\n",
       "      <td>0.184622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>0.482849</td>\n",
       "      <td>2672.53426</td>\n",
       "      <td>546.663930</td>\n",
       "      <td>112.006102</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.198099</td>\n",
       "      <td>0.116928</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>7.948668</td>\n",
       "      <td>2818.01707</td>\n",
       "      <td>...</td>\n",
       "      <td>7.745765</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>1.14492</td>\n",
       "      <td>0.149006</td>\n",
       "      <td>13.673940</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>6850.484442</td>\n",
       "      <td>45.745974</td>\n",
       "      <td>114.842372</td>\n",
       "      <td>21.978000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>617 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AB          AF          AH          AM         AR        AX  \\\n",
       "0    0.209377  3109.03329   85.200147   22.394407   8.138688  0.699861   \n",
       "1    0.145282   978.76416   85.200147   36.968889   8.138688  3.632190   \n",
       "2    0.470030  2635.10654   85.200147   32.360553   8.138688  6.732840   \n",
       "3    0.252107  3819.65177  120.201618   77.112203   8.138688  3.685344   \n",
       "4    0.380297  3733.04844   85.200147   14.103738   8.138688  3.942255   \n",
       "..        ...         ...         ...         ...        ...       ...   \n",
       "612  0.149555  3130.05946  123.763599    9.513984  13.020852  3.499305   \n",
       "613  0.435846  5462.03438   85.200147   46.551007  15.973224  5.979825   \n",
       "614  0.427300  2459.10720  130.138587   55.355778  10.005552  8.070549   \n",
       "615  0.363205  1263.53524   85.200147   23.685856   8.138688  7.981959   \n",
       "616  0.482849  2672.53426  546.663930  112.006102   8.138688  3.198099   \n",
       "\n",
       "           AY         AZ          BC         BD   ...         FI         FL  \\\n",
       "0    0.025578   9.812214    5.555634  4126.58731  ...   3.583450   7.298162   \n",
       "1    0.025578  13.517790    1.229900  5496.92824  ...  10.358927   0.173229   \n",
       "2    0.025578  12.824570    1.229900  5135.78024  ...  11.626917   7.709560   \n",
       "3    0.025578  11.053708    1.229900  4169.67738  ...  14.852022   6.122162   \n",
       "4    0.054810   3.396778  102.151980  5728.73412  ...  13.666727   8.153058   \n",
       "..        ...        ...         ...         ...  ...        ...        ...   \n",
       "612  0.077343   8.545512    2.804172  4157.68439  ...   9.879296   0.173229   \n",
       "613  0.025882  12.622906    3.777550  5654.07556  ...  10.910227  10.223150   \n",
       "614  0.025578  15.408390    1.229900  5888.87769  ...  12.029366   0.173229   \n",
       "615  0.025578   7.524588    1.229900  4517.86560  ...   8.026928   9.256996   \n",
       "616  0.116928   3.396778    7.948668  2818.01707  ...   7.745765   0.173229   \n",
       "\n",
       "           FR        FS         GB          GE            GF         GH  \\\n",
       "0     1.73855  0.094822  11.339138   72.611063   2003.810319  22.136229   \n",
       "1     0.49706  0.568932   9.292698   72.611063  27981.562750  29.135430   \n",
       "2     0.97556  1.198821  37.077772   88.609437  13676.957810  28.022851   \n",
       "3     0.49706  0.284466  18.529584   82.416803   2094.262452  39.948656   \n",
       "4    48.50134  0.121914  16.408728  146.109943   8524.370502  45.381316   \n",
       "..        ...       ...        ...         ...           ...        ...   \n",
       "612   1.26092  0.067730   8.967128  217.148554   8095.932828  24.640462   \n",
       "613   1.24236  0.426699  35.896418  496.994214   3085.308063  29.648928   \n",
       "614   0.49706  0.067730  19.962092  128.896894   6474.652866  26.166072   \n",
       "615   0.78764  0.670527  24.594488   72.611063   1965.343176  25.116750   \n",
       "616   1.14492  0.149006  13.673940   72.611063   6850.484442  45.745974   \n",
       "\n",
       "             GI         GL  \n",
       "0     69.834944   0.120343  \n",
       "1     32.131996  21.978000  \n",
       "2     35.192676   0.196941  \n",
       "3     90.493248   0.155829  \n",
       "4     36.262628   0.096614  \n",
       "..          ...        ...  \n",
       "612   69.191944  21.978000  \n",
       "613  124.808872   0.145340  \n",
       "614  119.559420  21.978000  \n",
       "615   37.155112   0.184622  \n",
       "616  114.842372  21.978000  \n",
       "\n",
       "[617 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train_data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predictor\n",
    "y=df['Class']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a naive model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The naive model would be that every patient has a 17.5% probability to have one of the three age related conditions. We will need to compute the balanced log loss for the naive model and use it as the baseline balanced log loss. Any other models that we make should have a balanced log loss that is less than that of our naive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.504051863857377"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_zero=509 #frequency of class zero\n",
    "class_one=108 #frequency of class one \n",
    "both=class_zero+class_one\n",
    "prob_of_one=class_one/(class_one+class_zero)\n",
    "prob_of_one_percentage=(class_one/(class_one+class_zero))*100\n",
    "prob_of_one_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.49594813614263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_of_zero=class_zero/(class_one+class_zero)\n",
    "prob_of_zero_percentage=(class_zero/(class_one+class_zero))*100\n",
    "prob_of_zero_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052, 0.17504052, 0.17504052, 0.17504052,\n",
       "       0.17504052, 0.17504052])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #This means we create an array of length freq_class_0 + freq_class_1 where each element is prob_class_1 .\n",
    "y_pred = np.array([prob_of_one] * both)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we are basically predicting class zero all the time \n",
    "y_pred_classes = np.array([0] * both)\n",
    "y_pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array([0] * class_zero + [1] * class_one)\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46378926223013367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normal log_loss\n",
    "normal_log_loss=log_loss(y_true,y_pred)\n",
    "normal_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced log loss formula\n",
    "def b_log_loss(y_t, y_p):\n",
    "    nc = np.bincount(y_true)\n",
    "    sample_weight = 1 / nc[y_true]\n",
    "    return log_loss(y_t, y_p, sample_weight=sample_weight, eps=1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9675794020680167"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#balanced log loss \n",
    "balanced_log_loss= b_log_loss(y_true, y_pred)\n",
    "balanced_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one column `EJ` is categorical. We will separate them and apply prepprocessing piplines to both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['EJ'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#numerical pipeline\n",
    "\n",
    "imputer=SimpleImputer(strategy='median')\n",
    "log_transformer=FunctionTransformer(np.log1p, inverse_func=np.expm1)\n",
    "scaler=StandardScaler()\n",
    "\n",
    "numerical_pipeline=Pipeline([\n",
    "    ('impute',imputer),\n",
    "    ('log transform',log_transformer),\n",
    "    ('scale', scaler)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical pipeline \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder=OneHotEncoder()\n",
    "cat_pipeline=Pipeline([\n",
    "    ('encoder',cat_encoder)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "\n",
    "\n",
    "X_preprocessing=ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline,make_column_selector(dtype_include=np.number)),\n",
    "    ('categorical',cat_pipeline, make_column_selector(dtype_include=object) )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not much so we will use k-fold  cross validation. We will make use of the pipeline to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear', class_weight='balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression training pipeline \n",
    "\n",
    "logreg_pipeline= Pipeline([\n",
    "    ('preprocess', X_preprocessing),\n",
    "    ('classifier',logreg)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation strategy\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, KFold\n",
    "cv=KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6857491089938375\n"
     ]
    }
   ],
   "source": [
    "processsed=X_preprocessing.fit_transform(X)\n",
    "model=logreg.fit(processsed,y)\n",
    "predicted_probs=model.predict_proba(processsed)\n",
    "balanced_log_loss= b_log_loss(y_true, predicted_probs)\n",
    "print(balanced_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8881685575364667\n",
      "Precision: 0.8003345498783455\n",
      "Recall: 0.8592738121225351\n"
     ]
    }
   ],
   "source": [
    "#other metrics: precision, accuracy, recall\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score\n",
    "\n",
    "predicted_labels = cross_val_predict(logreg_pipeline, X, y, cv=cv)\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "precision = precision_score(y, predicted_labels, average='macro')\n",
    "recall = recall_score(y, predicted_labels, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "dt= DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipeline=Pipeline([\n",
    "    ('preprocessing', X_preprocessing),\n",
    "    ('model',dt)\n",
    "    \n",
    "] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8833063209076175\n",
      "Precision: 0.7973014165321858\n",
      "Recall: 0.8016171869315287\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = cross_val_predict(dt_pipeline, X, y, cv=cv)\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "precision = precision_score(y, predicted_labels, average='macro')\n",
    "recall = recall_score(y, predicted_labels, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.250853417864832\n"
     ]
    }
   ],
   "source": [
    "processsed=X_preprocessing.fit_transform(X)\n",
    "model=dt.fit(processsed,y)\n",
    "predicted_probs=model.predict_proba(processsed)\n",
    "balanced_log_loss= b_log_loss(y_true, predicted_probs)\n",
    "print(balanced_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "knn=KNeighborsClassifier() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipeline=Pipeline([\n",
    "    ('preprocessing', X_preprocessing),\n",
    "    ('model',knn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.09901462077517\n"
     ]
    }
   ],
   "source": [
    "processsed=X_preprocessing.fit_transform(X)\n",
    "model=knn.fit(processsed,y)\n",
    "predicted_probs=model.predict_proba(processsed)\n",
    "balanced_log_loss= b_log_loss(y_true, predicted_probs)\n",
    "print(balanced_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.899513776337115\n",
      "Precision: 0.8868352223190933\n",
      "Recall: 0.7421414538310412\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = cross_val_predict(knn_pipeline, X, y, cv=cv)\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "precision = precision_score(y, predicted_labels, average='macro')\n",
    "recall = recall_score(y, predicted_labels, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember we cannot use rf and xgboost but we can uae them to compare with the dinal model we come up with \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf=RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline=Pipeline([\n",
    "    ('preprocessing',X_preprocessing),\n",
    "    ('model',rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6589696494581894\n"
     ]
    }
   ],
   "source": [
    "processsed=X_preprocessing.fit_transform(X)\n",
    "model=rf.fit(processsed,y)\n",
    "predicted_probs=model.predict_proba(processsed)\n",
    "balanced_log_loss= b_log_loss(y_true, predicted_probs)\n",
    "print(balanced_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9189627228525121\n",
      "Precision: 0.9072719127967747\n",
      "Recall: 0.7976970093865968\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = cross_val_predict(rf_pipeline, X, y, cv=cv)\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "precision = precision_score(y, predicted_labels, average='macro')\n",
    "recall = recall_score(y, predicted_labels, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgboost#check log loss, hence balanced log loss \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "xg_boost=GradientBoostingClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_pipeline=Pipeline([\n",
    "    ('preprocessing',X_preprocessing),\n",
    "    ('model',xg_boost)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.321568102315544\n"
     ]
    }
   ],
   "source": [
    "processsed=X_preprocessing.fit_transform(X)\n",
    "model=xg_boost.fit(processsed,y)\n",
    "predicted_probs=model.predict_proba(processsed)\n",
    "balanced_log_loss= b_log_loss(y_true, predicted_probs)\n",
    "print(balanced_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9222042139384117\n",
      "Precision: 0.8799377568040356\n",
      "Recall: 0.8397820708724442\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = cross_val_predict(xg_pipeline, X, y, cv=cv)\n",
    "accuracy = accuracy_score(y, predicted_labels)\n",
    "precision = precision_score(y, predicted_labels, average='macro')\n",
    "recall = recall_score(y, predicted_labels, average='macro')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using SMOTE \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
